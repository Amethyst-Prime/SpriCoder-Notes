lec3-评估的基础知识
---

1. 扩展
   1. 元宇宙
   2. UGC：User Generate Content
2. 违反设计原则的几个例子
   1. 南大学位论文管理系统：违反依赖用户记忆
   2. 用户不可以关闭界面：违反用户控制自由
   3. 卡住在解压资源：违反系统状态可见度
   4. 高级人机交互技术界面右下角没有找到按钮：一致性和标准化
   5. 成绩录入中的状态隐蔽：
   6. Google的界面越来越简化：第十条原则（审美与最小化设计）

# 1. 背景
1. 评估总是需要的
2. 什么是评估？
   1. 系统化的数据搜集过程
   2. 目的是了解用户或用户组在特定环境中，使用产品执行特定任务的情况
      1. 例如，用户能否找到特定的菜单项？图像是否有用，是否吸引人？产品是否引人入胜？
   3. 邀请用户进行评估的目的不是设法理解用户，而是评估特定用户在一个特定的环境背景中如何使用一个系统来执行一个特定的任务。

# 2. 关于评估的错误认识
1. 只有在系统开发完成后才需要评估：如果开发逾期，但是交付日期不变，那么评估部分就会被取消
2. 只要一个系统做了开发者认为是正确的事，那就足够了：案件检索的例子
3. 设计人员能够准确了解用户工作的方式
   1. 12306的例子
   2. 短信系统最初是为了让工程师调试手机的通信系统而设计的

# 3. 评估目标
1. 评估的优点
   1. 能够在交付产品之前（而不是之后）修复错误
   2. 设计小组能够专注于真实问题，而不是假想问题
   3. 工程师们能专心于编程而不是争论
   4. 能够大大缩短开发时间
   5. 销售部门可获得稳定的设计
2. 评估的目标
   1. 评估系统功能的范围和可达性（用户是否可以访问到）
   2. 评估交互中用户的体验
   3. 确定系统的某些特定问题

# 4. 评估原则
1. 评估应该依赖于产品的用户：与专业技术人员的水平和技术无关
2. 评估与设计应结合进行：仅靠用户最后对产品的**一两次评估**，是不能全面反映出软件可用性的
3. 评估应在用户的实际工作任务和操作环境下进行：根据用户完成任务的结果，进行客观的分析和评估
4. 要选择有广泛代表性的用户：参加测试的人必须具有代表性

# 5. 评估范型和技术
1. “范型”与“技术”
   1. 范型与具体学科相关，对如何评估有很大影响：可用性测试是一种评估范型
   2. 每种范型有特定的技术：可用性测试的技术有观察、问卷调查、访谈等
2. 评估范型
   1. 快速评估
   2. 可用性测试
   3. 实地研究
   4. 预测性评估

## 5.1. 快速评估
1. 设计人员非正式地向用户或顾问了解反馈信息，以证实设计构思是否符合用户需要
   1. 可在任何阶段进行
   2. 强调“快速了解”，而非仔细记录研究发现：如在设计初期了解用户对新产品的意见、在设计末期了解用户对图标设计的看法等
   3. 得到的数据通常是非正式、叙述性的：可以口语、书面笔记、草图、场景的形式反馈到设计过程
   4. 是设计网站时常用的方法
2. 基本特征：快速

## 5.2. 可用性测试
1. 20世纪80年代的主导方法
2. 评测典型用户执行典型任务时的情况：包括用户出错次数、完成任务的时间等
3. 基本特征：是在评估人员的密切控制之下实行的，是可以普适的。
4. 主要任务：量化表示用户的执行情况
5. 缺点
   1. 测试用户的数量通常较少
   2. 不适合进行细致的统计分析

## 5.3. 实地研究 Field Research
1. 基本特征：在自然工作环境中进行
2. 目的：理解用户的实际工作情形以及技术对他们的影响
3. 作用
   1. 探索新技术的应用契机
   2. 确定产品的需求
   3. 促进技术的引入
   4. 评估技术的应用
4. 分类
   1. 评测人员作为“局外人”
   2. 评测人员也可作为“局内人”或测试用户

## 5.4. 预测性评估
1. 专家们根据自己对典型用户的了解（通常使用启发式过程）预测可用性问题，也可使用理论模型
2. 基本特征
   1. **用户可以不在场**
   2. 使得整个过程快速、成本较低
3. 启发式评估是典型的预测性评估方法
   1. 注意：启发式原则应定制
   2. 可能误导设计人员：且有些结果可能并不准确

## 5.5. 评估范型比较
![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2021-human-computer-interaction/img/lec3/1.png)

## 5.6. 评估技术
1. 观察用户
   1. 有助于确定新产品的需求
   2. 也可用于评估原型
   3. 挑战：如何在不干扰用户的前提下观察用户，以及如何分析大量数据
2. 询问用户意见
   1. 简单，调查用户数量从几个到几百不等
3. 询问**专家意见**
   1. “角色扮演”方式评估
   2. 同时专家会提出解决方案
4. 测试用户的执行情况(UT, UserTest)
   1. 可比较不同设计方案优劣
   2. 通常在受控环境中进行
5. 基于模型和理论，预测界面的有效性
   1. 常用技术如GOMS模型和KLM模型等

## 5.7. 评估范型与技术的关系
![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2021-human-computer-interaction/img/lec3/2.png)

## 5.8. 区分评估技术的因素
1. 评估在周期中的位置：设计早期阶段的评估更快速、便宜
2. 评估的形式：实验室环境or工作环境
3. 技术的主客观程度
   1. 技术越主观，受评估人员知识的影响越大，如认知走查等
4. 测量的类型：与技术的主客观性有关
   1. 主观技术：定性数据
   2. 客观技术：定量数据
5. 提供的信息
   1. 低层信息：这个图标是可理解的吗？
   2. 高层信息：这个系统是可用的吗？
6. 响应的及时性
   1. 边做边说法可及时记录用户行为
   2. 任务后的走查取决于对事件的回忆
7. 干扰程度：直接响应测量可能会影响用户表现
8. 所需资源：设备、时间、资金、参与者、评估人员的专业技术及环境等

## 5.9. 评估技术比较
![](https://spricoder.oss-cn-shanghai.aliyuncs.com/2021-human-computer-interaction/img/lec3/3.png)

## 5.10. 评估方法组合
1. 评估方法的组合取决于项目待评估的具体特性
2. 常用组合
   1. 启发式评估+边做边说等用户测试技术
      1. 专家可通过启发性评估排除显而易见的可用性问题
      2. 重新设计后，经用户测试，反复检查设计的效果
   2. 访谈+问卷调查
      1. 先对小部分用户进行访谈，确定问卷中的具体问题
3. 启发式评估vs.用户测试
   1. 前者不需要用户参与
   2. 二者发现的可用性问题不同，可以互补